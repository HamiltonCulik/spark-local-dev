# Spark default configuration for development environment
# This file contains default configuration properties for Spark applications

# Spark Application Configuration
spark.app.name                     SparkDevApp
spark.master                       spark://spark-dev-master:7077

spark.driver.maxResultSize          1g

# Register the Delta Lake extension with Spark
spark.sql.extensions io.delta.sql.DeltaSparkSessionExtension

# Configure the Spark Catalog to use Delta's catalog
spark.sql.catalog.spark_catalog org.apache.spark.sql.delta.catalog.DeltaCatalog

# Dynamic Allocation (disabled for single-node setup)
spark.dynamicAllocation.enabled     false

# Serialization
spark.serializer                    org.apache.spark.serializer.KryoSerializer

# SQL Configuration
spark.sql.adaptive.enabled          true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.coalescePartitions.minPartitionNum  1
spark.sql.adaptive.advisoryPartitionSizeInBytes  64MB

# History Server
spark.eventLog.enabled              true
spark.eventLog.dir                  /opt/spark/spark-events
spark.history.fs.logDirectory       /opt/spark/spark-events

# UI Configuration
spark.ui.port                       4040
spark.ui.showConsoleProgress        true

# Development Settings
spark.sql.warehouse.dir             /opt/spark/work-dir/data/warehouse
spark.sql.shuffle.partitions        4

# Logging
spark.eventLog.compress             true
